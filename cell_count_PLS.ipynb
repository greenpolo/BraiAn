{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BraiAn\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Below, you have to specify:\n",
    "- ```group_1_name```: A meaningful string for Group 1.\n",
    "- ```group_2_name```: A meaningful string for Group 2.\n",
    "- ```group_1_names```: A list of names of the folders corresponding to animals in **Group 1** (e.g., Control group). Indeed, it is necessary to store the results in individual folders for each animal.\n",
    "- ```group_2_names```: A list of names of the folders corresponding to animals in **Group 2** (e.g., Stress group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"soumnya\"\n",
    "group_1_name = \"Control\"\n",
    "group_2_name = \"Stress\"\n",
    "output_folder = \"C-S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"soumnya\"\n",
    "group_1_name = \"Control (Females)\"\n",
    "group_2_name = \"Stress (Females)\"\n",
    "output_folder = \"CF-SF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"soumnya\"\n",
    "group_1_name = \"Control (Males)\"\n",
    "group_2_name = \"Stress (Males)\"\n",
    "output_folder = \"CM-SM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################### SET PARAMETERS ####################################\n",
    "\n",
    "data_input_path = f\"./data/experiments/{experiment}/BraiAn_norm_output/\"\n",
    "data_output_path = f\"./data/experiments/{experiment}/BraiAn_PLS_output/{output_folder}/\"\n",
    "plots_output_path = f\"./plots/{experiment}/{output_folder}/\"\n",
    "\n",
    "\n",
    "# ###########################################################################################\n",
    "\n",
    "\n",
    "if not(os.path.exists(data_output_path)):\n",
    "    os.makedirs(data_output_path, exist_ok=True)\n",
    "\n",
    "if not(os.path.exists(plots_output_path)):\n",
    "    os.makedirs(plots_output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1 = BraiAn.AnimalGroup.from_csv(group_1_name, data_input_path, f\"results_cell_counts_{group_1_name}.csv\")\n",
    "group_2 = BraiAn.AnimalGroup.from_csv(group_2_name, data_input_path, f\"results_cell_counts_{group_2_name}.csv\")\n",
    "if not group_1.is_comparable(group_2):\n",
    "    raise ImportError(\"Group 1 and Group 2 are not comparable!\\n\\\n",
    "Please check that you're reading two groups that normalized on the same brain regions and on the same marker\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are stored in ```group_1.data``` and ```group_2.data```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://help.brain-map.org/display/api/Downloading+an+Ontology%27s+Structure+Graph\n",
    "# StructureGraph id=1\n",
    "path_to_allen_json = \"./data/AllenMouseBrainOntology.json\"\n",
    "\n",
    "branches_to_exclude = [\"retina\", \"VS\", \"grv\", \"fiber tracts\", \"CB\"]\n",
    "AllenBrain = BraiAn.AllenBrainHierarchy(path_to_allen_json, branches_to_exclude)\n",
    "\n",
    "# Now, get the selected regions as a variable:\n",
    "level = 6\n",
    "\n",
    "#AllenBrain.unselect_all()\n",
    "#AllenBrain.select_at_structural_level(level)\n",
    "#selected_regions = AllenBrain.get_selected_regions()\n",
    "#print(f\"You selected {len(selected_regions)} regions at level {level}.\")\n",
    "#\n",
    "#AllenBrain.unselect_all()\n",
    "#AllenBrain.select_at_depth(level)\n",
    "#selected_regions = AllenBrain.get_selected_regions()\n",
    "#print(f\"You selected {len(selected_regions)} regions at depth {level}.\")\n",
    "#\n",
    "#AllenBrain.unselect_all()\n",
    "AllenBrain.select_from_csv(\"./data/AllenSummaryStructures.csv\")\n",
    "selected_regions = AllenBrain.get_selected_regions()\n",
    "print(f\"You selected {len(selected_regions)} Summary Structure regions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Least Squares  \n",
    "\n",
    "The analysis done below is taken from the tutorial written by [Krishnan et al.](https://www.sciencedirect.com/science/article/pii/S1053811910010074).  \n",
    "Run the 2 cells below to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLS\n",
    "normalization = \"RelativeDensity\" # group_1.get_normalization_methods() to know the available methods of group 1\n",
    "rank = 1\n",
    "\n",
    "# Create a PLS object\n",
    "pls = BraiAn.PLS(group_1, group_2, selected_regions, normalization)\n",
    "\n",
    "# Show the matrix X\n",
    "pls.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cross correlation analysis\n",
    "ds = pls.X.copy(deep=True)\n",
    "ds = ds.transpose()\n",
    "ds.index.name = \"BrainRegion\"\n",
    "ds.to_csv(os.path.join(data_output_path, f\"{group_1.marker.lower()}_{normalization.lower()}.csv\"), sep=\";\", decimal=\".\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the matrix Y\n",
    "pd.get_dummies(pls.y).rename(columns={0: group_2_name, 1: group_1_name})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two matrices printed above (X and Y) illustrate the data on which the PLS is done.  \n",
    "- ```X:``` The rows in this matrix are the mice. The columns in the matrix are the regions selected for analysis. The values in the matrix are the **normalized value of marked cells: in that region relative to the whole brain.** \n",
    "The normalization methods are either:\n",
    "  + Density\n",
    "  + Percentage (on the total number of detected marked cells outside of excluded regions)\n",
    "  + RelativeDensity\n",
    "- ```Y:``` The rows in this matrix are the mice. The columns in the matrix are the 2 groups. **A value in this matrix is 1 if the mice belongs to the specified group**.\n",
    "\n",
    "In brief, PLS analyzes the relationship (correlation) between the columns of ```X``` and ```Y```. In our specific case, there will be 2 important outputs:\n",
    "- **Salience scores**: Each brain region has a salience score. A high salience scores means that the brain region explains much of the correlation between ```X``` and ```Y```.  \n",
    "- **Singular values**: These are the eigenvalues of the correlation matrix $R = Y^TX$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random permutations to see whether we can differentiate signal from noise. \n",
    "Here, we randomly shuffle the group to which a mouse belongs, and calculate the singular values of the permuted dataset.  \n",
    "From [Krishnan et al.](https://www.sciencedirect.com/science/article/pii/S1053811910010074):  \n",
    "> The set of all the (permuted) singular values provides a sampling distribution of the singular values under the null hypothesis and, therefore can be used as a null hypothesis test.\n",
    "\n",
    "*Note: running the cell below will take a few minutes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_permutations = 5000\n",
    "print(f\"Randomly permuting singular values {num_permutations} times...\")\n",
    "s,singular_values = pls.randomly_permute_singular_values(num_permutations)\n",
    "# Plot distribution of singular values\n",
    "fig = BraiAn.plot_permutation(pls.s[0], singular_values, num_permutations)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate p-value = Probability(experiment | H0)\n",
    "p = (singular_values[:,0] > s[0]).sum() / num_permutations\n",
    "print(\"p-value = \"+str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap to identify stable salience scores\n",
    "\n",
    "Here, we use [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) (= sampling of the mice in the dataset, with replacement) to get an estimate of which salience scores are stable.\n",
    "\n",
    "From [Krishnan et al.](https://www.sciencedirect.com/science/article/pii/S1053811910010074):  \n",
    "> When a vector of saliences is considered generalizable and is kept for further analysis, we need to identify its elements that are stable through resampling. In practice, the stability of an element is evaluated by dividing it by its standard error. [...] To estimate the standard errors, we create bootstrap samples which are obtained by sampling with replacement the observations in and (Efron and Tibshirani, 1986). A salience standard error is then estimated as the standard error of the saliences from a large number of these bootstrap samples (say 1000 or 10000). **The ratios are akin to a Z-score, therefore when they are larger than 2 the corresponding saliences are considered significantly stable.**\n",
    "\n",
    "*Note: Running the cell below will take a few minutes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bootstrap = 5000\n",
    "print(f\"Bootstrapping salience scores {num_bootstrap} times...\")\n",
    "u_salience_scores,v_salience_scores = pls.bootstrap_salience_scores(rank, num_bootstrap)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PLS salience scores\n",
    "pls_salience_threshold = 1.2 # Only brain regions with a salience higher than plot_threshold are shown. 2 is the significance threshold.\n",
    "file_title = f\"PLS_{group_1.marker}_{normalization}.png\"\n",
    "tp, salient_regions = pls.plot_salience_scores(pls_salience_threshold, plots_output_path, file_title,\n",
    "                              fig_width=1000, fig_height=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salient_regions = salient_regions.reset_index()\n",
    "salient_regions.columns = [\"region\", \"salience\"]\n",
    "salient_regions[\"salience\"] = salient_regions[\"salience\"].abs()\n",
    "salient_regions = salient_regions.sort_values(by=\"salience\")\n",
    "salient_regions.to_csv(os.path.join(data_output_path, \"salient_regions.csv\"), sep=\";\", index=False)\n",
    "salient_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_filename = f\"PLS_{group_1.marker}_{normalization}_salience_scores.csv\"\n",
    "v_salience_scores = v_salience_scores.rename(columns={0:\"salience score\"})\n",
    "BraiAn.save_csv(v_salience_scores, data_output_path, pls_filename, overwrite=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_cutoff = 0.05\n",
    "r_cutoff = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_to_plot = BraiAn.regions_to_plot(pls, pls_salience_threshold)\n",
    "fig = BraiAn.plot_groups(normalization, AllenBrain, group_1, group_2,\n",
    "                            selected_regions=regions_to_plot, use_acronyms=False, height=5000)\n",
    "fig.show()\n",
    "\n",
    "file_title = f\"barplot_{group_1.marker}_{normalization}_{group_1.name}_vs_{group_2.name}.png\"\n",
    "fig.write_image(os.path.join(plots_output_path, file_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_cross_correlations = []\n",
    "for group in (group_1, group_2):\n",
    "    # min_animals=None because it doesn't matter. PLS already removes every region with NaNs.\n",
    "    r, p = group.cross_correlation(normalization, regions_to_plot, min_animals=None)\n",
    "    groups_cross_correlations.append((r, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, r,p in zip((group_1, group_2), groups_cross_correlations):\n",
    "    fig = BraiAn.draw_chord_plot(r, p, r_cutoff, p_cutoff, AllenBrain,\n",
    "                                ideograms_a=50,\n",
    "                                title=f\"{group.name} connectomics graph from Pearson correlation (|r| >= {r_cutoff}, p >= {p_cutoff})\",\n",
    "                                size=1200,\n",
    "                                no_background=False,\n",
    "                                annotation1=\"This is the first annotation\",\n",
    "                                annotation2=\"This is the second annotation\",\n",
    "                                annotation3=\"This is the third annotation\")\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "176c264d6092c25dbd928a202ffef3b4345a7b482eecd2272f203a3592feb37f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
