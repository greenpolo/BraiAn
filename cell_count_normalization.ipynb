{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABBA cell count analysis\n",
    "\n",
    "This notebook is the last step in the ABBA whole-brain cell counting analysis.  \n",
    "It assumes you have done the following steps:\n",
    "- Alignment of brain slices in ABBA, exported to a QuPath project.\n",
    "- Detected cells of interest in QuPath. The detections should be exported to ```.csv``` files (one per slice) in a folder called ```results```. \n",
    "- If there are regions to exclude, you should have drawn them and exported to ```.txt``` files (one per slice) in a folder called ```regions_to_exclude```.\n",
    "\n",
    "Run this notebook to load the cell counts and do analysis on them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we start ...\n",
    "The majority of the functions and classes we need written in 3 files: ```brain_hierarchy.py```, ```readCSV_helpers.py``` and ```pls_helpers.py```. We will now import the necessary functions and classes from these python files to this notebook, so that we can use them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from brain_hierarchy import AllenBrainHierarchy\n",
    "from readCSV_helpers import *\n",
    "from pls_helpers import PLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll need other python functions to easily read and manipulate data and make nice plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import copy\n",
    "# import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "# import seaborn as sns\n",
    "# import pickle\n",
    "import plotly.graph_objects as go\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Allen Brain Atlas\n",
    "\n",
    "We start by importing the mouse Allen Brain Atlas, in which we find information about all brain regions (their parent region and children regions in the brain hierarchy, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from https://help.brain-map.org/display/api/Downloading+an+Ontology%27s+Structure+Graph\n",
    "# StructureGraph id=1\n",
    "path_to_allen_json = \"./data/AllenMouseBrainOntology.json\"\n",
    "\n",
    "branches_to_exclude = ['retina','VS','grv','fiber tracts']\n",
    "AllenBrain = AllenBrainHierarchy(path_to_allen_json, branches_to_exclude)\n",
    "\n",
    "#edges = AllenBrain.edges_dict\n",
    "#tree = AllenBrain.tree_dict\n",
    "#brain_region_dict = AllenBrain.brain_region_dict\n",
    "#regions = list(brain_region_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the hierarchy of brain regions as a network (a tree). **Note that running the above cell may take a few minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot brain region hierarchy\n",
    "# If you want to plot it, install PyDot (pydot)\n",
    "fig = AllenBrain.plot_plotly_graph()\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graph above, you might want to specify the regions on which you want to do further PLS analysis:  \n",
    "*Note: to see more information about the regions, hover over them with your mouse.*\n",
    "\n",
    "- Specify a level. Analysis can only be done on one level (slice) in the brain region.\n",
    "\n",
    "- To exclude brain regions that belong to a certain branch, add the *abbreviated* nodes at the beginning of the branches to the list above.  \n",
    "Example:  \n",
    "```branches_to_exclude = ['retina', 'VS']```  \n",
    "means that **all the subregions that belong to the retina and the ventricular systems** are excluded from the PLS analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Now, we're ready to read the ```.csv``` files with the cell counts, and also the exclusion files (if there were regions to exclude).  \n",
    "Below, you have to specify:\n",
    "- ```animals_root```: Absolute path to the folder that contains the animal folders.\n",
    "- ```group_1_dirs```: A list of names of the folders corresponding to animals in **Group 1** (e.g., Control group). Indeed, it is necessary to store the results in individual folders for each animal.\n",
    "- ```group_2_dirs```: A list of names of the folders corresponding to animals in **Group 2** (e.g., Stress group).\n",
    "- ```group_1_name```: A meaningful string for Group 1.\n",
    "- ```group_2_name```: A meaningful string for Group 2.\n",
    "- ```area_key```: A string of the column in the ```.csv``` files that refers to the size of a brain areatra\n",
    "- ```tracer_key```: A string of the column in the ```.csv``` files that refers to the tracer number used to highlight the marker\n",
    "- ```marker_key```: A string of the marker we would like to highlight (e.g. CFos)\n",
    "\n",
    "Provare a modificar per ottenere densita in mm^2 (da micron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################### SET PARAMETERS ####################################\n",
    "\n",
    "\n",
    "animals_root = './data/QuPath_output/'\n",
    "group_1_dirs = ['Control_17C', 'Control_18C', 'Control_19C']\n",
    "group_1_name = 'Control'\n",
    "group_2_dirs = ['Stress_5S', 'Stress_8S', 'Stress_10S', 'Stress_13S', 'Resilient_1R', 'Resilient_2R', 'Resilient_3R', 'Resilient_4R', 'Resilient_11R']\n",
    "group_2_name = 'Stress'\n",
    "area_key = 'Area um^2'\n",
    "# area_key = 'DAPI: DAPI area um^2'\n",
    "tracer_key='Num AF647'\n",
    "marker_key='CFos'\n",
    "\n",
    "data_output_path = './data/python_norm_output/'\n",
    "plots_output_path = './plots/python_output/'\n",
    "\n",
    "\n",
    "# ###########################################################################################\n",
    "\n",
    "\n",
    "if not(os.path.exists(data_output_path)):\n",
    "    os.makedirs(data_output_path, exist_ok=True)\n",
    "if not(os.path.exists(plots_output_path)):\n",
    "    os.makedirs(plots_output_path, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the Control and Stress results seperately in two pandas dataframes, and save the results.\n",
    "\n",
    "**Note**: regions to exclude are automatically excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_group_slices(animal_root: str, animal_dirs: list[str], AllenBrain: AllenBrainHierarchy) -> list[list[pd.DataFrame]]:\n",
    "    animals_slices_paths = [os.path.join(animal_root, animal, 'results') for animal in animal_dirs]\n",
    "    animals_excluded_regions = [list_regions_to_exclude(os.path.join(animal_root, animal)) for animal in animal_dirs]\n",
    "    # load_cell_counts() -> list[pd.DataFrame], set[str], list[dict[str, str]]\n",
    "    return [load_cell_counts(input_path, exluded_regions, AllenBrain, area_key, tracer_key, marker_key)[0] for (input_path, exluded_regions) in zip(animals_slices_paths, animals_excluded_regions)]\n",
    "\n",
    "def area_µm2_to_mm2(group) -> None:\n",
    "    for slices in group:\n",
    "        for slice in slices:\n",
    "            slice.area = slice.area * 1e-06\n",
    "\n",
    "# for each brain region, aggregate marker counts from all the animal's slices into one value.\n",
    "# methods:\n",
    "# - sum\n",
    "# - avg & std of marker/area ratio (density)\n",
    "\n",
    "def sum_cell_counts(slices: list[pd.DataFrame]) -> pd.DataFrame:# methods: Callable[[int, int], int]):\n",
    "    slices_df = pd.concat(slices)\n",
    "    slices_df = slices_df.groupby(slices_df.index, axis=0).sum()\n",
    "    return slices_df\n",
    "\n",
    "def animal_cell_density(slices: list[pd.DataFrame], marker_key: str) -> pd.Series:\n",
    "    slices_marker_densities = [slice[marker_key] / slice['area'] for slice in slices]\n",
    "    return pd.concat(slices_marker_densities)\n",
    "\n",
    "def avg_cell_density(slices: list[pd.DataFrame], marker_key: str) -> pd.Series:\n",
    "    marker_densities = animal_cell_density(slices, marker_key)\n",
    "    avg_marker_densities = marker_densities.groupby(marker_densities.index, axis=0).mean()\n",
    "    return avg_marker_densities\n",
    "\n",
    "def std_cell_density(slices: list[pd.DataFrame], marker_key: str) -> pd.Series:\n",
    "    marker_densities = animal_cell_density(slices, marker_key)\n",
    "    std_marker_densities = marker_densities.groupby(marker_densities.index, axis=0).std()\n",
    "    return std_marker_densities[~std_marker_densities.isnull()] # remove NaN: when there is only one slice, the std can't be computed and instead outputs NaN\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Coefficient_of_variation\n",
    "def coefficient_variation(x) -> np.float64:\n",
    "    avg = x.mean()\n",
    "    if len(x) > 1 and avg != 0:\n",
    "        return x.std(ddof=1) / avg\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def variation_cell_density(slices: list[pd.DataFrame], marker_key: str) -> pd.Series:\n",
    "    # we merge hemisphere just in case there is some Right/Left region\n",
    "    merged_hem_slices = [merge_hemispheres(slice) for slice in slices]\n",
    "    marker_densities = animal_cell_density(merged_hem_slices, marker_key)\n",
    "    variation = marker_densities.groupby(marker_densities.index, axis=0).apply(coefficient_variation)\n",
    "    return variation\n",
    "\n",
    "# a: confidence threshold\n",
    "# TODO: determine k parameter (https://en.wikipedia.org/wiki/Normal_distribution#Confidence_intervals)\n",
    "# NOTE: avg not used atm\n",
    "def check_density_distribution(animals_slices: list[list[pd.DataFrame]], animal_names: list[str], a=0.001) -> None:\n",
    "    animals_avg_density = [avg_cell_density(brain_slices, marker_key) for brain_slices in animals_slices]\n",
    "    animals_std_density = [std_cell_density(brain_slices, marker_key) for brain_slices in animals_slices]\n",
    "    for i in range(len(animal_names)):\n",
    "        animal = animal_names[i]\n",
    "        region_in_confidence_interval = animals_std_density[i] < a\n",
    "        print(f\"Animal {animal}: out of {len(region_in_confidence_interval)} brain regions, {(~region_in_confidence_interval).sum()} are outside the confidence interval (a={a})\")\n",
    "        # print(region_in_confidence_interval.index[~region_in_confidence_interval])\n",
    "\n",
    "def write_brains(root_output_path: str, animal_names: list[str], animal_brains: list[pd.DataFrame]) -> None:\n",
    "    assert len(animal_names) == len(animal_brains),\\\n",
    "        f\"The number of animals read and analysed ({len(animal_brains)}) differs from the numner of animals in the input group ({len(animal_names)})\"\n",
    "    for i in range(len(animal_names)):\n",
    "        brain = animal_brains[i]\n",
    "        name = animal_names[i]\n",
    "        output_path = os.path.join(root_output_path, animal_names[i])\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        output_path = os.path.join(output_path, name+'_summed.csv')\n",
    "        brain.to_csv(output_path, sep='\\t', mode='w')\n",
    "        print(f'Raw summed cell counts are saved to {output_path}')\n",
    "\n",
    "def analyze(animal_names: list[str], animal_brains: list[pd.DataFrame], marker_key: str, AllenBrain: AllenBrainHierarchy) -> pd.DataFrame:\n",
    "    brain = pd.concat({name: normalize_cell_counts(brain, marker_key) for name,brain in zip(animal_names, animal_brains)})\n",
    "    brain = pd.concat({marker_key: brain}, axis=1)\n",
    "    brain = brain.reorder_levels([1,0], axis=0)\n",
    "    ordered_indices = product(AllenBrain.brain_region_dict.keys(), animal_names)\n",
    "    brain = brain.reindex(ordered_indices, fill_value=np.nan)\n",
    "    return brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change to group_1_slices / group_2_slices\n",
    "# NOTE: group_*_slices still discriminate Right from Left hemisphere. sort_hemispheres() to sum them.\n",
    "group_1_slices = read_group_slices(animals_root, group_1_dirs, AllenBrain)\n",
    "area_μm2_to_mm2(group_1_slices)\n",
    "print(f'Imported all brain slices from {str(len(group_1_slices))} animals of {group_1_name} group.')\n",
    "# check_density_distribution(group_1_slices, group_1_dirs, a=800)\n",
    "\n",
    "group_2_slices = read_group_slices(animals_root, group_2_dirs, AllenBrain)\n",
    "area_μm2_to_mm2(group_2_slices)\n",
    "print(f'Imported all brain slices  {str(len(group_2_slices))} animals of {group_2_name} group.')\n",
    "# check_density_distribution(group_2_slices, group_2_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLICE ANALYSIS: SUMMARY\n",
    "#\n",
    "# avg = avg_cell_density(group_1_slices[0], marker_key)\n",
    "# std = std_cell_density(group_1_slices[0], marker_key)\n",
    "# idx = variation_cell_density(group_1_slices[0], marker_key)\n",
    "# df = pd.concat(group_1_slices[0])\n",
    "# slices_per_area = df.groupby(df.index).count().iloc[:,0]\n",
    "# \n",
    "# threshold = 1\n",
    "# above_threshold_filter = idx > threshold\n",
    "# print(f\"\"\"Summary for animal 0:\n",
    "#     - N areas: {len(idx)}\n",
    "#     - Areas \\w CV > {threshold}:\n",
    "#         + N: {above_threshold_filter.sum()}\n",
    "#         + Mean slices/area: {slices_per_area[above_threshold_filter].mean()}\n",
    "#         + S.D. slices/area: {slices_per_area[above_threshold_filter].std()}\n",
    "#     - Areas \\w CV <= {threshold}:\n",
    "#         + N: {(~above_threshold_filter).sum()}\n",
    "#         + Mean slices/area: {slices_per_area[~above_threshold_filter].mean()}\n",
    "#         + S.D. slices/area: {slices_per_area[~above_threshold_filter].std()}\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_above_threshold(brains_CV, brains_name, marker_key, cv_threshold=1) -> go.Figure: \n",
    "    fig = go.Figure()\n",
    "    for i,cv in enumerate(brains_CV):\n",
    "        above_threshold_filter = cv > cv_threshold\n",
    "        # Scatterplot (animals)\n",
    "        fig.add_trace(go.Scatter(\n",
    "                            mode = 'markers',\n",
    "                            y = cv[above_threshold_filter],\n",
    "                            x = [i]*above_threshold_filter.sum(),\n",
    "                            text = cv.index[above_threshold_filter],\n",
    "                            opacity=0.7,\n",
    "                            marker=dict(\n",
    "                                size=7,\n",
    "                                line=dict(\n",
    "                                    color='rgb(0,0,0)',\n",
    "                                    width=1\n",
    "                                )\n",
    "                            ),\n",
    "                            showlegend=False\n",
    "                    )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title = f\"Coefficient of variaton of {marker_key} across brain slices > {cv_threshold}\",\n",
    "        \n",
    "        xaxis = dict(\n",
    "            tickmode = 'array',\n",
    "            tickvals = np.arange(0,len(brains_name)),\n",
    "            ticktext = brains_name\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title = \"Brain regions' CV\"\n",
    "        ),\n",
    "        width=700, height=500\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_selected_regions(regions_df: pd.DataFrame, AllenBrain: AllenBrainHierarchy) -> pd.DataFrame:\n",
    "    selected_allen_regions = AllenBrain.get_selected_regions()\n",
    "    selectable_regions = set(regions_df.index).intersection(set(selected_allen_regions))\n",
    "    return regions_df[list(selectable_regions)]\n",
    "\n",
    "group_1_CVs = [variation_cell_density(slices, marker_key) for slices in group_1_slices]\n",
    "AllenBrain.select_from_csv(\"./data/AllenSummaryStructures.csv\")\n",
    "group_1_CVs = [filter_selected_regions(brain, AllenBrain) for brain in group_1_CVs]\n",
    "\n",
    "group_2_CVs = [variation_cell_density(slices, marker_key) for slices in group_2_slices]\n",
    "AllenBrain.select_from_csv(\"./data/AllenSummaryStructures.csv\")\n",
    "group_2_CVs = [filter_selected_regions(brain, AllenBrain) for brain in group_2_CVs]\n",
    "\n",
    "cv_threshold = 1\n",
    "print(\"N regions above threshold:\", sum([(animal_cv > cv_threshold).sum() for animal_cv in group_1_CVs+group_2_CVs]))\n",
    "print(\"N regions below threshold:\", sum([(animal_cv <= cv_threshold).sum() for animal_cv in group_1_CVs+group_2_CVs]))\n",
    "plot_cv_above_threshold(group_1_CVs+group_2_CVs, group_1_dirs+group_2_dirs, marker_key, cv_threshold=cv_threshold).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 'IG'\n",
    "group = group_2_slices\n",
    "dirs = group_2_dirs\n",
    "i_animal = 1\n",
    "df = pd.concat([merge_hemispheres(slice) for slice in group[i_animal]])\n",
    "slices_per_area = df.groupby(df.index).count().iloc[:,0]\n",
    "print(f\"\"\"Summary for brain region '{r}' of {dirs[i_animal]}:\n",
    "    - N slices: {slices_per_area[r]}\n",
    "    - Coefficient of Variation: {variation_cell_density(group[i_animal], marker_key)[r]}\"\"\")\n",
    "#    - Mean: {avg_cell_density(group[i_animal], marker_key)[r]:.2f} {marker_key}/mm²),\n",
    "#    - S.D.: {std_cell_density(group[i_animal], marker_key)[r]:.2f} {marker_key}/mm²,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1_brains = [sum_cell_counts(cell_count_slices) for cell_count_slices in group_1_slices]\n",
    "# NOTE: brains are being written WITH Left/Right discrimination\n",
    "write_brains(data_output_path, group_1_dirs, group_1_brains)\n",
    "\n",
    "group_2_brains = [sum_cell_counts(cell_count_slices) for cell_count_slices in group_2_slices]\n",
    "write_brains(data_output_path, group_2_dirs, group_2_brains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fgh = group_1_brains[0].loc[['Left: root', 'Right: root']].sum()\n",
    "# fgh.CFos / fgh.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1_results = analyze(group_1_dirs, [merge_hemispheres(brain) for brain in group_1_brains], marker_key, AllenBrain)\n",
    "group_2_results = analyze(group_2_dirs, [merge_hemispheres(brain) for brain in group_2_brains], marker_key, AllenBrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "save_results(group_1_results, data_output_path, f'results_cell_counts_{group_1_name}.csv')\n",
    "save_results(group_2_results, data_output_path, f'results_cell_counts_{group_2_name}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d344e578332e1bb60027361a1c4829827483114257e284778c747ec1e64c3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
